---
layout: page
title: Tokenization, lemmatization, frequency calculation
---

This module includes:
1. **Tokenization** – splitting raw text into words and punctuation  
2. **Lemmatization** – reducing each word to its dictionary form  
3. **Frequency calculation** – counting token occurrences  

---

### 1. Tokenization

Tokenization means turning a raw string into a clean list of tokens (words and punctuation).
In the previous classes, we learned that tokenization is an important step for different types of text analysis.

#### 1-1. Simple `.split()`

```python
text = "Hello, world! This is Python."
# Naïve split:
tokens = text.split(" ")
print(tokens)
# → ['Hello,', 'world!', 'This', 'is', 'Python.']
````

Notice punctuation sticks to words. We can insert spaces around punctuation then split:

```python
def tokenize_simple(text):
    for p in [".", ",", "!", "?", ";", ":"]:
        text = text.replace(p, f" {p} ")
    return text.lower().split()

print(tokenize_simple(text))
# → ['hello', ',', 'world', '!', 'this', 'is', 'python', '.']
```

#### 1-2. Regex-based tokenization (optional)

Regular expressions (regex) are patterns that describe sets of strings. They let you *match*, *search*, and *split* text using rules (e.g., “match one or more word characters” or “match any punctuation mark.”)

```python
import re

def tokenize_regex(text):
    # \w+ matches words, [^\w\s] matches any punctuation
    return re.findall(r"\w+|[^\w\s]", text.lower())

print(tokenize_regex(text))
# → ['hello', ',', 'world', '!', 'this', 'is', 'python', '.']
```

#### 1-3. Toolkit-based tokenization (NLTK)

NLTK includes a robust tokenizer that handles edge cases like abbreviations and contractions:

```python
import nltk
from nltk.tokenize import word_tokenize

# On first run, download the tokenizer models:
# nltk.download('punkt')

def tokenize_nltk(text):
    return word_tokenize(text.lower())

text = "Hello, world! This is Python."
print(tokenize_nltk(text))
# → ['hello', ',', 'world', '!', 'this', 'is', 'python', '.']
```

**Why use NLTK’s tokenizer?**

* Handles English contractions (e.g., “don’t” → \["do", "n’t"])
* Preserves multi-character punctuation (e.g., “...” remains “...”)
* Ready-made, well-tested for many languages and genres

#### 1-4. Toolkit-based tokenization (spaCy)

spaCy is an industrial-strength NLP library that provides fast, accurate tokenization (and full pipelines for tagging, parsing, and more).

```bash
# Install spaCy and the small English model
pip install spacy
python -m spacy download en_core_web_sm
````

```python
import spacy

# Load the English model
nlp = spacy.load("en_core_web_sm")

def tokenize_spacy(text):
    """
    Tokenize text using spaCy's rule-based tokenizer.
    Returns a list of token strings.
    """
    doc = nlp(text)
    return [token.text for token in doc]

text = "Hello, world! This is Python."
print(tokenize_spacy(text))
# → ['Hello', ',', 'world', '!', 'This', 'is', 'Python', '.']
```

**Why use spaCy?**

* **Accuracy**: handles punctuation, contractions, and special cases out of the box
* **Speed**: optimized Cython implementation for large corpora
* **Extensibility**: integrates with lemmatization, part-of-speech tagging, named-entity recognition, etc.

---

### 2. Lemmatization

Lemmatization means converting each token to its base (dictionary) form.

### 2-1 Dictionary-based lemmatizer

```python
# Suppose lemma_dict = {"is":"be", "ran":"run", ...}
def lemmatize_dict(tokens, lemma_dict):
    return [lemma_dict.get(tok, tok) for tok in tokens]

tokens = ["this", "is", "running"]
lemma_dict = {"is":"be", "running":"run"}
print(lemmatize_dict(tokens, lemma_dict))
# → ['this', 'be', 'run']
```

#### 2-2 NLTK’s WordNetLemmatizer

```python
import nltk
from nltk.stem import WordNetLemmatizer

wnl = WordNetLemmatizer()
print(wnl.lemmatize("running", pos="v"))   # → 'run'
print(wnl.lemmatize("better", pos="a"))    # → 'good'
```

#### 2-3 spaCy Lemmatizer

```python
import spacy

# Load English model (once)
nlp = spacy.load("en_core_web_sm")

def lemmatize_spacy(text):
    """
    Tokenize and lemmatize with spaCy.
    Returns list of lemmas in input order.
    """
    doc = nlp(text)
    return [token.lemma_ for token in doc]

sample = "Running faster makes you better."
print(lemmatize_spacy(sample))
# → ['run', 'fast', 'make', 'you', 'good', '.']
```

---

### 3. Frequency Calculation

Text data analysis often starts with counting the number of tokens in the text, which means counting how many times each word token (or lemma) appears.

#### 3-1 Using `collections.Counter`

```python
from collections import Counter

tokens = ["a", "b", "a", ".", "a", "b"]
freq = Counter(tokens)
print(freq)
# → Counter({'a': 3, 'b': 2, '.': 1})

# Top 3 most common:
print(freq.most_common(3))
# → [('a', 3), ('b', 2), ('.', 1)]
```

#### 3-2 Simple loop version

```python
def freq_simple(tokens):
    counts = {}
    for tok in tokens:
        counts[tok] = counts.get(tok, 0) + 1
    return counts

print(freq_simple(tokens))
```

---

### 4. End-to-End: `corpus_freq`

Combine all steps to process every `.txt` file in a folder.

```python
import re
import glob
from collections import Counter
from nltk.stem import WordNetLemmatizer

wnl = WordNetLemmatizer()

def tokenize(text):
    return re.findall(r"\w+|[^\w\s]", text.lower())

def lemmatize(tokens):
    return [wnl.lemmatize(tok, pos="v") for tok in tokens]

def corpus_freq(folder):
    total = Counter()
    for path in glob.glob(f"{folder}/*.txt"):
        text = open(path, encoding="utf-8", errors="ignore").read()
        toks = tokenize(text)
        lems = lemmatize(toks)
        total.update(lems)
    return total

# Example usage:
freq = corpus_freq("my_corpus")
print(freq.most_common(10))
```

---

### 5. Exercises

1. **Custom Tokenizer**

   * Modify `tokenize()` to treat apostrophes as part of words (e.g., “don’t” → “don’t” not “don” / “‘t”).
   * Test on `"Don't split me!"`.

2. **Stopword Filtering**

   * Using NLTK’s stopword list (`nltk.corpus.stopwords.words('english')`), filter out common words before frequency counting.
   * Compare the top 10 tokens with and without stopwords.

3. **Batch Lemmatizer**

   * Write `lemmatize_dict(tokens, lemma_dict)` that falls back to WordNet if a token isn’t in `lemma_dict`.
   * Evaluate on a small sample.

4. **Frequency TSV Output**

   * Write a function `write_freq_tsv(freq, filename, top_n=None)` that writes `word<TAB>count` per line, optionally only the top *n* items.
   * Generate `top_50.tsv` for your corpus.

5. **Corpus Comparison**

   * Run `corpus_freq` on two different folders (e.g., `news/` vs. `fiction/`).
   * Create a bar chart (using matplotlib) of the 5 most distinctive words in each (highest relative frequency difference).

---